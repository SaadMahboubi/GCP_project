# ğŸš€ Automatisation et gestion des workloads Data sur GCP

## ğŸ“Œ Description
Projet personnel de **migration et traitement des donnÃ©es sur Google Cloud Platform (GCP)**, intÃ©grant **BigQuery, GKE, Cloud Composer, Terraform et Kubernetes** pour automatiser et optimiser les workflows data.

## ğŸ”¹ Stack & Technologies
- **GKE (Google Kubernetes Engine)** : DÃ©ploiement & orchestration des workloads.
- **BigQuery** : Stockage, transformation et optimisation des donnÃ©es.
- **Cloud Composer (Airflow)** : Orchestration des pipelines de donnÃ©es.
- **Terraform** : DÃ©ploiement automatisÃ© de lâ€™infrastructure GCP.
- **GCS (Google Cloud Storage)** : Stockage des DAGs Airflow.
- **Python, SQL, Kubernetes, CI/CD**.

## ğŸš€ Features & Actions mises en place
- ğŸ“Œ **DÃ©ploiement Terraform** : Cluster GKE avec autoscaling, Cloud Composer.
- ğŸ“Œ **Automatisation BigQuery** : Ingestion et transformation via DAGs Airflow.
- ğŸ“Œ **Optimisation SQL** : Partitionnement, Clustering, Tables intermÃ©diaires.
- ğŸ“Œ **Orchestration GKE** : `KubernetesPodOperator`, `CronJobs` pour requÃªtes BigQuery.
- ğŸ“Œ **Gestion des DAGs** : Stockage & versioning sur GCS.
- ğŸ“Œ **Monitoring avancÃ©** : Debugging avec `kubectl logs`, suivi des coÃ»ts BigQuery.

## ğŸ› ï¸ Setup & Installation
**Cloner le repo**  
```bash
git clone https://github.com/ton-repo.git && cd ton-repo
```

**TransfÃ©rer les DAGs Airflow dans le Bucket GCS de Cloud Composer**
```bash
gsutil cp airflow/k8s_airflow_test.py gs://europe-west1-test-composer--5658f951-bucket/dags/

gsutil cp airflow/bigquery_query_execution.py gs://europe-west1-test-composer--5658f951-bucket/dags/

gsutil cp airflow/load_gcs_to_bigquery.py gs://europe-west1-test-composer--5658f951-bucket/dags/
```

**VÃ©rifier que les fichiers ont bien Ã©tÃ© envoyÃ©s**
```bash
gsutil ls gs://europe-west1-test-composer--5658f951-bucket/dags/
```

**Envoyer un fichier CSV dans Google Cloud Storage (GCS)**
```bash
gsutil cp airflow/my_data.csv gs://europe-west1-test-composer--5658f951-bucket/data/
```

**VÃ©rifier que le fichier a bien Ã©tÃ© envoyÃ©**
```bash
gsutil cp airflow/my_data.csv gs://europe-west1-test-composer--5658f951-bucket/data/
```

**Avant dâ€™exÃ©cuter Terraform, charge le fichier .env pour le composer**
```bash
source .env
terraform init
terraform apply -var="gcp_project_id=$GCP_PROJECT_ID" -var="gcp_region=$GCP_REGION" -var="composer_env_name=$COMPOSER_ENV_NAME"
```

**Avant dâ€™exÃ©cuter Terraform, charge le fichier .env pour GKE**
```bash
source .env
terraform init
terraform apply -var="gcp_project_id=$GCP_PROJECT_ID" \
                -var="gcp_region=$GCP_REGION" \
                -var="gke_cluster_name=$GKE_CLUSTER_NAME" \
                -var="gke_location=$GKE_LOCATION" \
                -var="gke_node_pool_name=$GKE_NODE_POOL_NAME" \
                -var="gke_machine_type=$GKE_MACHINE_TYPE" \
                -var="gke_min_nodes=$GKE_MIN_NODES" \
                -var="gke_max_nodes=$GKE_MAX_NODES" \
                -var="gke_disk_size=$GKE_DISK_SIZE"
```

**DÃ©ployer l'infrastructure avec Terraform** assurez vous d'Ãªtre dans le bon dossier (composer-terraform ou gke-terraform)
```bash
terraform init && terraform apply
```

**VÃ©rifier les workloads Kubernetes**
```bash
kubectl get pods
```