# 🚀 Automatisation et gestion des workloads Data sur GCP

## 📌 Description
Projet personnel de **migration et traitement des données sur Google Cloud Platform (GCP)**, intégrant **BigQuery, GKE, Cloud Composer, Terraform et Kubernetes** pour automatiser et optimiser les workflows data.

## 🔹 Stack & Technologies
- **GKE (Google Kubernetes Engine)** : Déploiement & orchestration des workloads.
- **BigQuery** : Stockage, transformation et optimisation des données.
- **Cloud Composer (Airflow)** : Orchestration des pipelines de données.
- **Terraform** : Déploiement automatisé de l’infrastructure GCP.
- **GCS (Google Cloud Storage)** : Stockage des DAGs Airflow.
- **Python, SQL, Kubernetes, CI/CD**.

## 🚀 Features & Actions mises en place
- 📌 **Déploiement Terraform** : Cluster GKE avec autoscaling, Cloud Composer.
- 📌 **Automatisation BigQuery** : Ingestion et transformation via DAGs Airflow.
- 📌 **Optimisation SQL** : Partitionnement, Clustering, Tables intermédiaires.
- 📌 **Orchestration GKE** : `KubernetesPodOperator`, `CronJobs` pour requêtes BigQuery.
- 📌 **Gestion des DAGs** : Stockage & versioning sur GCS.
- 📌 **Monitoring avancé** : Debugging avec `kubectl logs`, suivi des coûts BigQuery.

## 🛠️ Setup & Installation
**Cloner le repo**  
```bash
git clone https://github.com/ton-repo.git && cd ton-repo
```

**Transférer les DAGs Airflow dans le Bucket GCS de Cloud Composer**
```bash
gsutil cp airflow/k8s_airflow_test.py gs://europe-west1-test-composer--5658f951-bucket/dags/

gsutil cp airflow/bigquery_query_execution.py gs://europe-west1-test-composer--5658f951-bucket/dags/

gsutil cp airflow/load_gcs_to_bigquery.py gs://europe-west1-test-composer--5658f951-bucket/dags/
```

**Vérifier que les fichiers ont bien été envoyés**
```bash
gsutil ls gs://europe-west1-test-composer--5658f951-bucket/dags/
```

**Envoyer un fichier CSV dans Google Cloud Storage (GCS)**
```bash
gsutil cp airflow/my_data.csv gs://europe-west1-test-composer--5658f951-bucket/data/
```

**Vérifier que le fichier a bien été envoyé**
```bash
gsutil cp airflow/my_data.csv gs://europe-west1-test-composer--5658f951-bucket/data/
```

**Avant d’exécuter Terraform, charge le fichier .env pour le composer**
```bash
source .env
terraform init
terraform apply -var="gcp_project_id=$GCP_PROJECT_ID" -var="gcp_region=$GCP_REGION" -var="composer_env_name=$COMPOSER_ENV_NAME"
```

**Avant d’exécuter Terraform, charge le fichier .env pour GKE**
```bash
source .env
terraform init
terraform apply -var="gcp_project_id=$GCP_PROJECT_ID" \
                -var="gcp_region=$GCP_REGION" \
                -var="gke_cluster_name=$GKE_CLUSTER_NAME" \
                -var="gke_location=$GKE_LOCATION" \
                -var="gke_node_pool_name=$GKE_NODE_POOL_NAME" \
                -var="gke_machine_type=$GKE_MACHINE_TYPE" \
                -var="gke_min_nodes=$GKE_MIN_NODES" \
                -var="gke_max_nodes=$GKE_MAX_NODES" \
                -var="gke_disk_size=$GKE_DISK_SIZE"
```

**Déployer l'infrastructure avec Terraform** assurez vous d'être dans le bon dossier (composer-terraform ou gke-terraform)
```bash
terraform init && terraform apply
```

**Vérifier les workloads Kubernetes**
```bash
kubectl get pods
```